{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d0ecdb12-7777-4a22-9ce2-b268e072335d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/5\n",
      "   9088/3999988 [..............................] - ETA: 33:59:37 - loss: 0.0483 - accuracy: 0.0000e+00  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Bidirectional, LSTM, Dropout, BatchNormalization\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_absolute_percentage_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "# Load dataset (Replace with actual file path if needed)\n",
    "data = pd.read_csv(\"sales_5000000.csv\")\n",
    "\n",
    "# Use only the first 10,000 rows\n",
    "data = data.head(100000)\n",
    "\n",
    "# Convert Order Date to datetime and sort data\n",
    "data[\"Order Date\"] = pd.to_datetime(data[\"Order Date\"])\n",
    "data = data.sort_values(by=\"Order Date\")\n",
    "\n",
    "# Extract year and add as a feature\n",
    "data[\"Year\"] = data[\"Order Date\"].dt.year\n",
    "\n",
    "# Select relevant columns\n",
    "time_series = data[[\"Order Date\", \"Year\", \"Total Profit\"]].set_index(\"Order Date\")\n",
    "\n",
    "# Split data into training and testing sets\n",
    "year_split = data[\"Year\"].quantile(0.8)  # 80% of data for training\n",
    "train_data = time_series[time_series[\"Year\"] <= year_split]\n",
    "test_data = time_series[time_series[\"Year\"] > year_split]\n",
    "\n",
    "# Normalize the profit values\n",
    "scaler = MinMaxScaler()\n",
    "train_data[\"Total Profit\"] = scaler.fit_transform(train_data[[\"Total Profit\"]])\n",
    "test_data[\"Total Profit\"] = scaler.transform(test_data[[\"Total Profit\"]])\n",
    "\n",
    "# Create sequences\n",
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length])\n",
    "        y.append(data[i+seq_length])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Define sequence length\n",
    "seq_length = 20\n",
    "\n",
    "# Prepare data for model\n",
    "values_train = train_data[\"Total Profit\"].values\n",
    "values_test = test_data[\"Total Profit\"].values\n",
    "\n",
    "X_train, y_train = create_sequences(values_train, seq_length)\n",
    "X_test, y_test = create_sequences(values_test, seq_length)\n",
    "\n",
    "X_train = X_train.reshape((X_train.shape[0], X_train.shape[1], 1))\n",
    "X_test = X_test.reshape((X_test.shape[0], X_test.shape[1], 1))\n",
    "\n",
    "# Build Improved BiLSTM model\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(100, return_sequences=True), input_shape=(seq_length, 1)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Bidirectional(LSTM(100, return_sequences=True)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Bidirectional(LSTM(50)),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.2),\n",
    "    Dense(256, activation='relu'),\n",
    "    Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae', 'mse'])\n",
    "\n",
    "# Early stopping to prevent overfitting\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
    "\n",
    "# Train model\n",
    "history = model.fit(X_train, y_train, epochs=50, batch_size=16, validation_data=(X_test, y_test), callbacks=[early_stopping])\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# Inverse transform predictions\n",
    "y_test_inv = scaler.inverse_transform(y_test.reshape(-1, 1))\n",
    "y_pred_inv = scaler.inverse_transform(y_pred)\n",
    "\n",
    "# Compute evaluation metrics (adding small constant to avoid division errors)\n",
    "mape = mean_absolute_percentage_error(y_test_inv + 1e-9, y_pred_inv + 1e-9) * 100\n",
    "r2 = r2_score(y_test_inv, y_pred_inv)\n",
    "accuracy = 100 - mape  # Approximate accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac6fed0f-aaf7-40b4-9e5a-c3f4f7b691aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
